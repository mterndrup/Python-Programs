import os
import time
from Bio.Blast import NCBIWWW, NCBIXML
from Bio import SeqIO, Entrez
import threading
import sys
from datetime import datetime
import csv
import glob

barcode = input("Enter the barcode number (e.g., 05 for barcode05): ").strip().zfill(2)
primertype = "FITS"

input_dir = (r"C:\Users\ketgl\OneDrive\Documents\GitHub\Python-Programs\BioTech"
             r"\Bioinformatics\Chaparral\wildfirePlants-DNA-nanopore-sequence"
             fr"\fastq_pass\barcode{barcode}")

# Now the input fasta file (no fastq or cutadapt steps)
input_fasta_path = os.path.join(input_dir, f"barcode{barcode}_{primertype}.fasta")

if not os.path.exists(input_fasta_path):
    sys.exit(f"âŒ Input FASTA file not found: {input_fasta_path}")

identity_threshold = 85
Entrez.api_key = "b1ee6a94722846e3fe858612221221948607"
Entrez.email = "terndrupm@gmail.com"

folder_name = os.path.basename(input_dir.rstrip(os.sep))

# Output files
fasta_blast_output_path = os.path.join(input_dir, f"{folder_name}_{primertype}_BLAST.fasta")

log_files = sorted(glob.glob(os.path.join(input_dir, f"{folder_name}_{primertype}_blast_log_*.txt")))

if log_files:
    log_output_path = log_files[-1]
    print(f"âš¡ Resuming from existing log: {log_output_path}")
else:
    log_output_path = os.path.join(input_dir, f"{folder_name}_{primertype}_blast_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.txt")
    print(f"ğŸ†• Starting new log file: {log_output_path}")

summary_csv_path = os.path.join(input_dir, f"{folder_name}_sum.csv")

species_counts = {}
processed_ids = set()

if os.path.exists(fasta_blast_output_path):
    for record in SeqIO.parse(fasta_blast_output_path, "fasta"):
        processed_ids.add(record.id)

if os.path.exists(log_output_path):
    with open(log_output_path, "r", encoding="utf-8") as log_file:
        header_line = log_file.readline()
        for line in log_file:
            if line.strip():
                rec_id = line.split("\t")[0]
                processed_ids.add(rec_id)

if os.path.exists(summary_csv_path):
    with open(summary_csv_path, "r", encoding="utf-8") as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            species_counts[row["Species"]] = int(row["Count"])

def blast_with_timeout(seq, timeout=300):
    blast_result = [None]

    def blast_task():
        try:
            print("ğŸ” Starting BLAST request...")
            result_handle = NCBIWWW.qblast("blastn", "nt", seq, hitlist_size=1,
                                           entrez_query="txid4751[ORGN]")
            return result_handle
        except Exception as e:
            print(f"âŒ Exception during BLAST: {e}")
            return e

    thread = threading.Thread(target=lambda: blast_result.__setitem__(0, blast_task()))
    thread.start()

    start_time = time.time()
    while thread.is_alive():
        elapsed = time.time() - start_time
        if elapsed > timeout:
            print(f"â° Timeout after {timeout} seconds (sequence length: {len(seq)}). Marking as skipped.")
            return None
        time.sleep(0.1)

    thread.join(timeout=1)
    result = blast_result[0]

    if result is None:
        print("âš ï¸ No result returned from BLAST thread â€” possible crash or premature exit.")
        return None

    if isinstance(result, Exception):
        raise result

    return result

def extract_genus_species(description):
    parts = description.split()
    if len(parts) >= 2:
        return parts[0], f"{parts[0]} {parts[1]}"
    elif parts:
        return parts[0], parts[0]
    return "unknown", "unknown"

def is_fungi_kingdom_or_uncultured(hit_def, accession):
    if "uncultured fungus" in hit_def.lower():
        return True
    try:
        handle = Entrez.efetch(db="nucleotide", id=accession, rettype="gb", retmode="xml")
        records = Entrez.read(handle)
        handle.close()
        if not records:
            return False
        lineage = records[0].get("GBSeq_taxonomy", "")
        return "Fungi" in lineage
    except Exception as e:
        print(f"  âš ï¸ Taxonomy lookup error for {accession}: {e}")
        return False

with open(fasta_blast_output_path, "a") as fasta_out, \
     open(log_output_path, "a", encoding="utf-8") as log_file:

    if os.path.getsize(log_output_path) == 0:
        log_file.write("ID\tStatus\tReason\tTop_Hit\n")

    for record in SeqIO.parse(input_fasta_path, "fasta"):
        if record.id in processed_ids:
            print(f"â­ï¸ Skipping already-processed: {record.id}")
            continue

        seq = record.seq
        seq_len = len(seq)
        timeout = min(480, max(300, int(seq_len / 200)))

        print(f"\nğŸ”¬ BLASTing {record.id} (len: {seq_len}, timeout: {timeout}s)")
        try:
            result_handle = blast_with_timeout(seq, timeout=timeout)
        except Exception as e:
            reason = f"BLAST error: {e}"
            print(f"  âŒ {reason}")
            log_file.write(f"{record.id}\tExcluded\t{reason}\tN/A\n")
            continue

        if result_handle is None:
            reason = "User skipped or timeout"
            print(f"  â© {reason}")
            log_file.write(f"{record.id}\tExcluded\t{reason}\tN/A\n")
            continue

        try:
            blast_record = NCBIXML.read(result_handle)
        except Exception as e:
            reason = f"Parse error: {e}"
            print(f"  âŒ {reason}")
            log_file.write(f"{record.id}\tExcluded\t{reason}\tN/A\n")
            continue

        if blast_record.alignments and blast_record.alignments[0].hsps:
            top_hit = blast_record.alignments[0]
            hsp = top_hit.hsps[0]
            hit_def = top_hit.hit_def
            accession = top_hit.accession
            identity_pct = (hsp.identities / hsp.align_length) * 100
            genus, genus_species = extract_genus_species(hit_def)

            print(f"  ğŸ” Top Hit: {hit_def}")
            print(f"  ğŸ”— Identity: {identity_pct:.1f}%")

            is_fungal = is_fungi_kingdom_or_uncultured(hit_def, accession)

            if is_fungal and identity_pct >= identity_threshold:
                header = f"{genus_species} ~{int(identity_pct)}%"
                fasta_out.write(f">{record.id}\n{seq}\n")
                species_counts[genus_species] = species_counts.get(genus_species, 0) + 1
                print(f"  âœ… Included as: {header}")
                log_file.write(f"{record.id}\tIncluded\tFungal\t{hit_def} ~{int(identity_pct)}%\n")
            else:
                reason = "Low identity" if is_fungal else "Non-fungal"
                print(f"  âŒ Excluded ({reason})")
                log_file.write(f"{record.id}\tExcluded\t{reason}\t{hit_def} ~{int(identity_pct)}%\n")
        else:
            reason = "No alignment"
            print(f"  âŒ {reason}")
            log_file.write(f"{record.id}\tExcluded\t{reason}\tN/A\n")

        processed_ids.add(record.id)
        time.sleep(3)

# Write Summary CSV
with open(summary_csv_path, "w", newline='', encoding='utf-8') as csvfile:
    fieldnames = ["Species", "Count"]
    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
    writer.writeheader()
    for species, count in sorted(species_counts.items(), key=lambda x: x[1], reverse=True):
        writer.writerow({"Species": species, "Count": count})

print(f"\nğŸ“Š Summary CSV saved: {summary_csv_path}")
print("âœ… Process completed.")
sys.exit(0)
